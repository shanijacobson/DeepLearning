# -*- coding: utf-8 -*-
"""Project-Ron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qeimhBS15XylGgYO_7BkepB_Rqy6lFcy
"""

# Commented out IPython magic to ensure Python compatibility.
from tokenize import Double
import torch
import os
import sys
import numpy as np
from torch.utils.data import DataLoader, random_split

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
DEVICE = torch.device("cpu")

os.chdir(r"C:\Users\Ron\Desktop\DeepLearning\Project")
DATA_PATH = os.path.join("Data", "Phoenix14")
emotions_val= os.path.join("Data", "Phoenix14","emothins.pkl")
emotions_train= os.path.join("Data", "Phoenix14","emothins_train.pkl")
SAVED_MODELS_PATH = "Data/models"
VALIDATION_SIZE = 520
BATCH_SIZE = 32
MINIMUM_LR = 1.0e-07
MAXIMUM_ITER = 5_000_000
SEED = 42

torch.manual_seed(SEED)

# %cd gdrive/MyDrive/DeepLearing/Project
import Models
# %cd /content

"""## **Initialization**"""

# Commented out IPython magic to ensure Python compatibility.
# %cd gdrive/MyDrive/DeepLearing/Project

"""## **Load Data**"""

# Commented out IPython magic to ensure Python compatibility.
from Models import Vocabulary
from Models import SignGlossLanguage

# %cd gdrive/MyDrive/DeepLearing/Project

# Build Vocabularies:
gloss_vocab = Vocabulary.GlossVocabulary(DATA_PATH)
word_vocab = Vocabulary.WordVocabulary(DATA_PATH)

# Build Datasets

train_dataset = SignGlossLanguage(root=DATA_PATH, type="train", download=False,
                                  word_vocab=word_vocab, gloss_vocab=gloss_vocab, emotions_path=emotions_train)
valid_dataset = SignGlossLanguage(root=DATA_PATH, type="dev", download=False,
                                  word_vocab=word_vocab, gloss_vocab=gloss_vocab,emotions_path = emotions_val)
test_dataset = SignGlossLanguage(root=DATA_PATH, type="test", download=False,
                                 word_vocab=word_vocab, gloss_vocab=gloss_vocab)

train_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)
validation_loader = DataLoader(valid_dataset, BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False)

"""## **Training**"""
from Models import SLTModel, SLTModelLoss, Helper, Predictions
from torch.optim import Adam, lr_scheduler
from torch.utils.data.dataloader import T
from torchtext.data.metrics import bleu_score
from torchmetrics.functional import word_error_rate
from torch.utils.tensorboard import SummaryWriter

def calculate_validation_scores(beam_size=None, alpha=0, test=False, iter=0):
    model.eval()
    idx_to_words = word_vocab.get_itos()
    idx_to_glosses = gloss_vocab.get_itos()
    seq_to_words = {}
    total_loss = 0
    total_recognition_loss = 0
    total_translation_loss = 0
    total_emo_loss = 0

    with torch.no_grad():
        txt_ref = []
        txt_hyp = []
        gls_ref = []
        gls_hyp = []
        data_loader = test_loader if test else validation_loader
        for batch in data_loader:
            (frames, frames_len), (glosses, glosses_len), (words, words_len) , emo= batch
            (frames, glosses, words, emo) = (frames.to(DEVICE), glosses.to(DEVICE), words.to(DEVICE), emo.to(DEVICE))
            words_output, glosses_output, encoder_output, emo_prob_output = model(frames, words)

            # calculate loss
            loss, recognition_loss, translation_loss, emo_loss =  criterion(glosses, words, glosses_output, words_output, frames_len, glosses_len,emo,emo_prob_output)
            total_loss += loss
            total_recognition_loss += recognition_loss
            total_translation_loss += translation_loss
            total_emo_loss += emo_loss


            # predict words and glosses
         #   real_words_list = [[[idx_to_words[seq[i+1]] for i in range(30) if seq[i+1] != word_vocab[Vocabulary.PAD_TOKEN]]] 
                               # for seq in words]
          #  txt_ref.extend(real_words_list)
          #  real_gloss_list = [Predictions.clean_gloss_output([idx_to_glosses[seq[i+1]] for i in range(glosses.shape[1] - 1) if seq[i+1] != gloss_vocab[Vocabulary.PAD_TOKEN]]) 
                                #for seq in glosses]                    
           # gls_ref.extend(real_gloss_list)
            #predict_words_list = Predictions.predict_words(model, frames, words, encoder_output, word_vocab, beam_size, alpha)
            #txt_hyp.extend(predict_words_list)
            #predict_glosses_list = Predictions.predict_glosses(idx_to_glosses, glosses_output, frames_len, beam_size=1)
            #gls_hyp.extend(predict_glosses_list)

    validation_bleu_score = 0 #bleu_score(txt_hyp, txt_ref, max_n=4)
    validation_wer_score = 1 # word_error_rate(gls_hyp, gls_ref)
    total_loss /= VALIDATION_SIZE
    total_recognition_loss /= VALIDATION_SIZE
    total_translation_loss /= VALIDATION_SIZE
    total_emo_loss /= VALIDATION_SIZE
    print(f"total_emo_loss : {total_emo_loss * criterion.emo_loss_weight },total_recognition_loss : {total_recognition_loss * criterion.gloss_loss_weight},total_translation_loss : {total_translation_loss * criterion.word_loss_weight}" )

    if not test:
        writer.add_scalar("total_loss/validation", total_loss, iter)
        writer.add_scalar("recognition_loss/validation", total_recognition_loss, iter) 
        writer.add_scalar("translation_loss/validation", total_translation_loss, iter)       
        writer.add_scalar('bleu/validation', validation_bleu_score, iter) 
        writer.add_scalar('wer/validation', validation_wer_score, iter) 
    return total_loss, validation_bleu_score, validation_wer_score


def train_on_batch(batch, iter):
    model.train()
    (frames, frames_len), (glosses, glosses_len), (words, words_len), emo = batch
    (frames, glosses, words, emo) = (frames.to(DEVICE), glosses.to(DEVICE), words.to(DEVICE), emo.to(DEVICE))
    words_output, glosses_output, encoder_output, emo_prob_output = model(frames, words)
    total_loss, recognition_loss, translation_loss,emo_loss = criterion(glosses, words, glosses_output, words_output, frames_len, glosses_len,emo,emo_prob_output)
    norm_loss = (total_loss.double() / glosses.shape[0]).double()
    optimizer.zero_grad()
    norm_loss.backward()
    optimizer.step()
    writer.add_scalar("total_loss/train", total_loss / frames.shape[0], iter)
    writer.add_scalar("recognition_loss/train", recognition_loss / frames.shape[0], iter) 
    writer.add_scalar("translation_loss/train", translation_loss / frames.shape[0], iter) 
    return total_loss


def init_running_state(model_name):
    iter = 0
    best_lr = optimizer.param_groups[0]["lr"]
    best_bleu_score = -np.inf
    best_validation_loss = np.inf
    best_wer_score = np.inf
 #   state = Helper.restore_iteration_state(model_name)
 #   if state is not None:   
#        model.load_state_dict(state["model"])
#        optimizer.load_state_dict(state["optimizer"])
 #       scheduler.load_state_dict(state["scheduler"])
  #      iter = state["iteration"]
   #     best_lr = state["best_learning_rate"]
    #    best_bleu_score = state["best_bleu_score"]
     #   best_wer_score = state["best_wer_score"]
      #  best_validation_loss = state["best_validation_loss"]
    return iter, best_lr, best_bleu_score, best_wer_score, best_validation_loss


def fit(model_name):
    iter, best_lr, best_bleu_score, best_wer_score, best_validation_loss = init_running_state(model_name)
    
    stop = False
    epoch = 0
    
    while not stop:
        epoch_loss = 0
        for batch in train_loader:
            if (iter) % 1 == 0:

                validation_loss, validation_bleu_score, validation_wer_score = calculate_validation_scores(iter=iter)
                prev_lr = optimizer.param_groups[0]["lr"]
                scheduler.step(validation_bleu_score) 
                new_lr = optimizer.param_groups[0]["lr"]
                if validation_bleu_score > best_bleu_score:
                    print(f"new best validation bleu score! validation loss: {validation_loss}, bleu score: {validation_bleu_score}, wer score: {validation_wer_score}")
                    best_bleu_score = validation_bleu_score
                    torch.save(model.state_dict(), f"{SAVED_MODELS_PATH}/{model_name}/best_bleu_score_model_state")
                    best_lr = new_lr
                if validation_loss < best_validation_loss:
                    best_validation_loss = validation_loss
                    torch.save(model.state_dict(), f"{SAVED_MODELS_PATH}/{model_name}/best_loss_model_state")
                if validation_wer_score < best_wer_score :
                    best_wer_score = validation_wer_score
                    print(f"new best validation wer score! validation loss: {validation_loss}, bleu score: {validation_bleu_score}, wer score: {validation_wer_score}")
                    torch.save(model.state_dict(), f"{SAVED_MODELS_PATH}/{model_name}/best_wer_score_model_state")
                if prev_lr != new_lr:
                    print(f"new lr: {new_lr}")
                    if  best_bleu_score > 0.12 and best_lr != prev_lr:
                        stop = True
            epoch_loss += train_on_batch(batch, iter)
            if optimizer.param_groups[0]["lr"] < MINIMUM_LR or iter > 100:
                stop = True
                break
            iter += 1
        #print(f"Epoch: {epoch}, Train loss: {epoch_loss / len(train_dataset)}, Validation loss: {validation_loss}")
        if (epoch) % 20:
            Helper.save_running_state(model_name,
                                      iteration=iter,
                                      best_learning_rate=best_lr,
                                      best_bleu_score=best_bleu_score,
                                      best_wer_score=best_wer_score,
                                      best_validation_loss=best_validation_loss,
                                      model_state_dict=model.state_dict(),
                                      optimizer_state_dict=optimizer.state_dict(),
                                      scheduler_state_dict=scheduler.state_dict())
        epoch += 1


model = SLTModel(frame_size=1024, gloss_dim=len(gloss_vocab), words_dim=len(word_vocab), 
                num_layers_encoder=2, num_layers_decoder=2, emo_dim= 7,
                word_padding_idx=word_vocab[Vocabulary.PAD_TOKEN]).to(DEVICE)
optimizer = Adam(model.parameters(), lr=0.001, 
                weight_decay=0.001, eps=1.0e-8, amsgrad=False)
scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode="max", threshold_mode="abs", 
                                            verbose=False, factor=0.5, patience=8)
criterion = SLTModelLoss(gloss_blank_index=gloss_vocab[Vocabulary.SIL_TOKEN], 
                            gloss_loss_weight = 0, word_loss_weight = 0, emo_loss_weight=1,
                            word_ignore_index=word_vocab[Vocabulary.PAD_TOKEN]).to(DEVICE)
writer = SummaryWriter(log_dir = f"runs/test")                     
#calculate_validation_scores(1)
fit("test")
writer.flush()#fit()

