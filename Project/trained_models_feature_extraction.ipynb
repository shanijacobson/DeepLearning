{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ronsh\\anaconda3\\envs\\cuda\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "import time\n",
    "import re \n",
    "from deepface import DeepFace\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = r\"./Data/Phoenix14/\"\n",
    "poses_names = [(\"poses_train.pkl\",\"train\"), (\"poses_val.pkl\",\"dev\"), (\"poses_test.pkl\",\"test\")]\n",
    "emotions_names = [(\"emotions_train.pkl\",\"train\"), (\"emotions_val.pkl\",\"val\"), (\"emotrion_test.pkl\",\"test\")]\n",
    "raw_imgs_path = r\".\\Data\\features\\PHOENIX-2014-T-release-v3\\PHOENIX-2014-T\\features\\fullFrame-210x260px\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extraction Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def extract_poses_from_samples(poses = None, img_path = \"\", dict_name = \"poses\"):\n",
    "    paths = os.listdir(img_path)\n",
    "    paths = list(filter(lambda x: len(x.split(\".\")) == 1, paths))\n",
    "    if poses is not None:\n",
    "        paths = list(filter(lambda x: x not in poses.keys(),paths))\n",
    "        print(f\"number of folders processed is {len(poses.keys())}, nunber of folders left is {len(paths)}\")\n",
    "\n",
    "    else:\n",
    "        poses = {}\n",
    "   \n",
    "    mp_pose = mp.solutions.pose\n",
    "    for index, folder in enumerate(paths):\n",
    "        print(folder)\n",
    "        start = time.time()\n",
    "        pose = mp_pose.Pose(min_detection_confidence = 0.3,min_tracking_confidence = 0.3, model_complexity=2,\n",
    "                enable_segmentation=True)\n",
    "        if index % 10 == 0:\n",
    "            with open(r'./Data/Phoenix14/{name}'.format(name=dict_name), 'wb') as f:\n",
    "                pickle.dump(poses, f, pickle.HIGHEST_PROTOCOL)\n",
    "                print(f\"folder index : {index} , saved!\")\n",
    "       \n",
    "        poses[folder] = []\n",
    "        imgs = os.listdir(img_path +\"/\" + folder)\n",
    "       \n",
    "        for img in imgs:\n",
    "          frame = cv2.imread(img_path +\"/\" + folder + \"/\" + img)\n",
    "          results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "          landmark_list = str(results.pose_landmarks).split(\"landmark\")[1:]\n",
    "          landmark_cord = [tuple(re.findall(\"-?\\d.\\d*\", landmark)[:-1]) for landmark in landmark_list]\n",
    "          poses[folder].append(landmark_cord)\n",
    "       \n",
    "        end = time.time()\n",
    "        total_time = end - start\n",
    "        print(\"\\n\"+ str(total_time))\n",
    "    return poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emotions_from_samples(emothins = None, path = \"\",num_of_emo = 7,dict_name = \"emothin\"):\n",
    "  paths = os.listdir(path)\n",
    "  paths = list(filter(lambda x: len(x.split(\".\")) == 1, paths))\n",
    "  if emothins is not None:\n",
    "    paths = list(filter(lambda x: x not in emothins.keys(),paths))\n",
    "    print(f\"number of folders processed is {len(emothins.keys())}, nunber of folders left is {len(paths)}\")\n",
    "  else:\n",
    "    emothins = {}\n",
    "  \n",
    "  for index, folder in enumerate(paths):\n",
    "    print(folder)\n",
    "    if index % 10 == 0:\n",
    "      with open(r'./Data/Phoenix14/{name}'.format(name=dict_name), 'wb') as f:\n",
    "          pickle.dump(emothins, f, pickle.HIGHEST_PROTOCOL)\n",
    "          print(f\"folder index : {index} , saved!\")\n",
    "\n",
    "    emothins[folder] = []\n",
    "    imgs = os.listdir(path +\"/\" + folder)\n",
    "    for img in imgs:\n",
    "      try:\n",
    "        result = list(DeepFace.analyze(img_path = f\"./{path}/{folder}/{img}\", \n",
    "            actions = ['emotion'])[\"emotion\"].values())\n",
    "        result = torch.tensor(result) / 100\n",
    "      except Exception as e:\n",
    "        #print(e)\n",
    "        result = torch.tensor([1/num_of_emo for _ in range(num_of_emo)])\n",
    "     \n",
    "      emothins[folder].append(result)\n",
    "  return emothins "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Emotaions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dict\n",
      "number of folders processed is 2, nunber of folders left is 0\n"
     ]
    }
   ],
   "source": [
    "for dict_name , folder in emotions_names:\n",
    "    \n",
    "     cur_path = dict_path + dict_name\n",
    "     if os.path.exists(cur_path):\n",
    "          with open(cur_path, 'rb') as f:\n",
    "               print(\"loading dict\")\n",
    "               emothins = pickle.load(f)\n",
    "     \n",
    "          emothins = extract_emotions_from_samples(emothins,path = os.path.join(raw_imgs_path,folder),dict_name=dict_name)\n",
    "     else:\n",
    "          emothins = extract_emotions_from_samples(path = os.path.join(raw_imgs_path,folder),dict_name=dict_name)\n",
    "\n",
    "     with  open(cur_path, 'wb') as f:\n",
    "           pickle.dump(emothins, f, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Poses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dict\n",
      "number of folders processed is 2, nunber of folders left is 0\n"
     ]
    }
   ],
   "source": [
    "for dict_name , folder in poses_names:\n",
    "   \n",
    "     cur_path = dict_path + dict_name\n",
    "     if os.path.exists(cur_path):\n",
    "          with open(cur_path, 'rb') as f:\n",
    "               print(\"loading dict\")\n",
    "               poses = pickle.load(f)\n",
    "     \n",
    "          poses = extract_poses_from_samples(poses,img_path = os.path.join(raw_imgs_path,folder),dict_name=dict_name)\n",
    "     else:\n",
    "          poses = extract_poses_from_samples(img_path = os.path.join(raw_imgs_path,folder),dict_name=dict_name)\n",
    "\n",
    "     with  open(cur_path, 'wb') as f:\n",
    "           pickle.dump(poses, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('cuda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce282ddbccad15b0233ee04ddee93d51d2e29c43c02554c3b0802b3600493beb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
