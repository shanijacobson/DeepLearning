{"cells":[{"cell_type":"markdown","metadata":{"id":"d32XCcZ4mTU4"},"source":["\n","# **LSTM for Penn-Bankset**\n"]},{"cell_type":"markdown","metadata":{"id":"nJ_ijLKKmkoL"},"source":["## **Initialization**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nK3Sw0AfQAFj"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.optim import SGD, lr_scheduler\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kz9X3hgeQBov"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","torch.manual_seed(101)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3754,"status":"ok","timestamp":1651647512954,"user":{"displayName":"Shani Jacobson","userId":"17202169386450009800"},"user_tz":-180},"id":"KSZEQHXSvpGu","outputId":"cdff06e2-a422-4522-f1f8-2177b01fc8ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AyUQ-vDnPPGg"},"outputs":[],"source":["PATH = \"/content/drive/MyDrive/ex2_313581803_314882861\"\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","EMBEDDING_SIZE = 200\n","HIDDEN_SIZE = 200\n","NUM_OF_LAYERS = 2\n","BATCH_SIZE = 20 \n","SEQUENCE_SIZE = 20\n","INIT_WEIGHT = 0.1"]},{"cell_type":"markdown","metadata":{"id":"Tisketk3mv-u"},"source":["### **Load Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HRUTursNXGGE"},"outputs":[],"source":["def word_to_ix(word):\n","  if word not in vocab_map:\n","    vocab_map[word] = len(vocab_map)\n","  return vocab_map[word]\n","   \n","def split_minibatch(ix_list, batch_size):\n","  # add only full minibatchs\n","  total_seq_size = len(ix_list) // batch_size\n","  return np.array(\n","      [np.array(ix_list[total_seq_size*i: total_seq_size*(i+1)]) for i in range(batch_size)]) \n"," \n","def load_data(set_type, batch_size=BATCH_SIZE):\n","  with open(f\"{PATH}/Data/ptb.{set_type}.txt\",\"r\") as f:\n","    sentences = f.read().split(\"\\n\")\n","    sentences = [sen for sen in sentences if len(sen) != 0]\n","    word_list = \"<eos>\".join(sentences).split(\" \")\n","    ix_list = [word_to_ix(word) for word in word_list if word != \"\"]\n","    return split_minibatch(ix_list, batch_size)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1782,"status":"ok","timestamp":1651647544903,"user":{"displayName":"Shani Jacobson","userId":"17202169386450009800"},"user_tz":-180},"id":"c5rOnZOpBUjU","outputId":"070bce4b-901f-40fb-9ee9-f51649a2ca2f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 10000\n"]}],"source":["vocab_map = {}\n","\n","train_set = load_data(\"train\")\n","valid_set = load_data(\"valid\")\n","test_set = load_data(\"test\")\n","print(f\"Vocabulary size: {len(vocab_map)}\")"]},{"cell_type":"markdown","metadata":{"id":"lg0yz0QdnEum"},"source":["### **Network Architecture**\n","\n","Implement LSTM for Penn-Bankset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"btXYA3EzZDH8"},"outputs":[],"source":["class RNN(nn.Module):\n","    def __init__(self, vocab_size, p=0, use_gru=False):\n","        super(RNN, self).__init__()\n","        self.hidden_size = EMBEDDING_SIZE\n","        self.num_layers = NUM_OF_LAYERS\n","        self.use_gru = use_gru\n","        self.word2Embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n","        self.dropout = nn.Dropout(p)\n","        self.lstm = nn.LSTM(input_size=EMBEDDING_SIZE, hidden_size=self.hidden_size, \n","                            num_layers=self.num_layers, dropout=p, batch_first=True)\n","        self.gru=nn.GRU(EMBEDDING_SIZE, self.hidden_size, self.num_layers, dropout=p, batch_first=True)\n","        self.hidden2word = nn.Linear(EMBEDDING_SIZE, vocab_size)\n","        self.criterion = nn.CrossEntropyLoss(reduction='sum')\n","        self.init_weights()\n","\n","\n","    def forward(self, sentence):\n","        embeds = self.word2Embedding(torch.tensor(sentence))\n","        embeds = self.dropout(embeds)\n","        if self.use_gru:\n","          rnn_out, h_n = self.gru(embeds, self.h_0)\n","          self.h_0 = h_n.detach()\n","        else:\n","          rnn_out, (h_n, c_n) = self.lstm(embeds, (self.h_0, self.c_0))\n","          self.h_0, self.c_0 = h_n.detach(), c_n.detach()\n","        rnn_out = self.dropout(rnn_out)\n","        words_pred = self.hidden2word(rnn_out)\n","        return words_pred\n","\n","    def init_state(self):\n","        self.h_0 = torch.zeros(self.num_layers, BATCH_SIZE, self.hidden_size).to(DEVICE)\n","        self.c_0 = torch.zeros(self.num_layers, BATCH_SIZE, self.hidden_size).to(DEVICE)\n","\n","    def init_weights(self):\n","        self.word2Embedding.weight.data.uniform_(-INIT_WEIGHT, INIT_WEIGHT)\n","        self.hidden2word.bias.data.fill_(0)\n","        self.hidden2word.weight.data.uniform_(-INIT_WEIGHT, INIT_WEIGHT)\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hARKnSlxMG2A"},"outputs":[],"source":["def fit(model, train_data, val_data, optimizer, scheduler, max_norm, file_name, \n","        max_epochs=50, save_model=False):\n","    train_results = []\n","    val_results = []\n","    best_loss = 1000000000\n","    for epoch in range(max_epochs):\n","      model.init_state()\n","      model.train()\n","      for i in range(0, train_data.shape[1], SEQUENCE_SIZE):\n","          # Forward pass\n","          end_of_seq = min(i+SEQUENCE_SIZE,train_data.shape[1]-1)\n","          seq = torch.tensor(train_set[:,i:end_of_seq]).to(DEVICE)\n","          labels = torch.tensor(train_set[:,i+1:end_of_seq+1]).to(DEVICE)\n","          y_pred = model(seq)\n","          loss = model.criterion(y_pred.transpose(dim0=1,dim1=2), labels)\n","\n","          # Propagating the loss backward and optimizing the parameters\n","          optimizer.zero_grad()\n","          loss.backward()\n","          nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n","          optimizer.step()\n","      scheduler.step()\n","      \n","      train_results.append(evaluation(model, train_data))\n","      val_results.append(evaluation(model, val_data))\n","      if save_model and val_results[-1] < best_loss :\n","        torch.save(model.state_dict(),f\"{PATH}/Models/{file_name}_best.pt\")\n","        best_acc = val_results[-1]\n","      print(f\"epoch: {epoch}, train loss: {train_results[-1]}, validation loss: {val_results[-1]} \\\n","            train perplexity: {np.exp(train_results[-1]):10.8f}, validation perplexity: {np.exp(val_results[-1]):10.8f}.\") \n","    if save_model:\n","        torch.save(model.state_dict(),f\"{PATH}/Models/{file_name}_final.pt\")\n","    generate_plots(train_results, val_results, file_name)\n","    return train_results[-1], val_results[-1]\n","\n","\n","def evaluation(model, data):\n","    model.eval()\n","    total_loss = 0\n","    total_pred = 0\n","    model.init_state()\n","    with torch.no_grad():\n","      for i in range(0, data.shape[1], SEQUENCE_SIZE):\n","          end_of_seq = min(i+SEQUENCE_SIZE, data.shape[1]-1)\n","          if i == end_of_seq:\n","            break\n","          seq = torch.tensor(data[:,i:end_of_seq]).to(DEVICE)\n","          labels = torch.tensor(data[:,i+1:end_of_seq+1]).to(DEVICE)\n","          y_pred = model(seq)\n","          loss = model.criterion(y_pred.transpose(dim0=1,dim1=2), labels)\n","          total_loss += float(loss)\n","          total_pred += BATCH_SIZE * (end_of_seq - i)\n","    return total_loss / total_pred\n","\n","\n","def generate_plots(train_results, val_results, file_name):\n","    if not os.path.exists(f\"{PATH}/Results\"):\n","      os.mkdir(f\"{PATH}/Results\")\n","    plt.xlabel(\"epoch\")\n","    plt.ylabel(\"loss\")\n","    plt.plot(np.exp(np.array(train_results)), label='training Perplexity', linewidth=5.0)\n","    plt.plot(np.exp(np.array(val_results)), label='validation Perplexity', linewidth=5.0)\n","    plt.title('Perplexity per epoch')\n","    plt.legend()\n","    plt.grid()\n","\n","    plt.savefig(f'{PATH}/Results/{file_name}.png', bbox_inches='tight')\n","    plt.close()"]},{"cell_type":"markdown","metadata":{"id":"YSKY8mPK9x89"},"source":["## **Find Hyperparameters**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tevM0RKQdTgU"},"outputs":[],"source":["def find_hyper_parameters(mode, lr_list, decay_threshold_list, decay_list, max_norm_list,\n","                          p_lists, use_gru, init_whight, max_epochs):\n","  results = {}\n","  if not os.path.exists(f\"{PATH}/Results/{mode}\"):\n","    os.makedirs(f\"{PATH}/Results/{mode}\")\n","  if not os.path.exists(f\"{PATH}/Models/{mode}\"):\n","    os.makedirs(f\"{PATH}/Models/{mode}\")\n","  for lr in lr_list:\n","    for decay_threshold in decay_threshold_list:\n","      for decay in decay_list:\n","        for max_norm in max_norm_list:\n","          for p in p_lists:\n","            model = RNN(len(vocab_map), p=p, use_gru=use_gru, init_whight=init_whight).to(DEVICE)\n","            file_name = f\"{mode}/{lr}_{decay_threshold}_{decay}_{max_norm}_{p}\"\n","            optimizer = SGD(model.parameters(), lr=lr)\n","            scheduler = lr_scheduler.LambdaLR(optimizer, lambda epoch: 1 if epoch < decay_threshold else decay**(-epoch+decay_threshold))\n","            results[(lr, decay_threshold, decay, max_norm, p)] = fit(\n","                model, train_set, valid_set, optimizer, scheduler, max_norm, file_name=file_name, save_model=True, max_epochs=max_epochs)\n","\n","  np.save(f\"{PATH}/Results/{mode}/res_dic\", results, allow_pickle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_7WDGqr7Hlq"},"outputs":[],"source":["################################\n","### Find LSTM Hyperparmeters ###\n","################################\n","\n","mode = \"LSTM\"\n","lr_list = [1.5,1.2,1,0.8,0.5]\n","decay_threshold_list = [1,3,5]\n","decay_list = [3,2,1.5]\n","max_norm_list = [5]\n","p_list = [0]\n","find_hyper_parameters(mode, lr_list, decay_threshold_list, decay_list, max_norm_list, p_list, \n","                      use_gru=False, init_whight=0.1, max_epochs=20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-GlOTsw7Hjy"},"outputs":[],"source":["########################################\n","### Find LSTM-Dropout Hyperparmeters ###\n","########################################\n","\n","mode = \"LSTM_Dropout\"\n","lr_list = [1.2,1,0.8]\n","decay_threshold_list = [5,7,10]\n","decay_list = [2,1.5,1.05]\n","max_norm_list = [5]\n","p_list = [0.35,0.5,0.65]\n","find_hyper_parameters(mode, lr_list, decay_threshold_list, decay_list, max_norm_list, p_list, \n","                      use_gru=False, init_whight=0.05, max_epochs=50)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mKIQgLEP7SYt"},"outputs":[],"source":["###############################\n","### Find GRU Hyperparmeters ###\n","###############################\n","\n","mode = \"GRU\"\n","lr_list = [1,1.2,1.5]\n","decay_threshold_list = [1,3,5]\n","decay_list = [3,2,1.5]\n","max_norm_list = [2]\n","p_list = [0]\n","find_hyper_parameters(mode, lr_list, decay_threshold_list, decay_list, max_norm_list, p_list, \n","                      use_gru=True, init_whight=0.1, max_epochs=20)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hryOTgD77Wvz"},"outputs":[],"source":["\n","#######################################\n","### Find GRU Dropout Hyperparmeters ###\n","#######################################\n","\n","mode = \"GRU_Dropout\"\n","lr_list = [1,1.2,1.5]\n","decay_threshold_list = [5,7,10]\n","decay_list = [2,1.3,1.08]\n","max_norm_list = [2]\n","p_list = [0.35,0.5,0.65]\n","find_hyper_parameters(mode, lr_list, decay_threshold_list, decay_list, max_norm_list, p_list, \n","                      use_gru=True, init_whight=0.1, max_epochs=50)"]},{"cell_type":"markdown","metadata":{"id":"-XnQ1iePFGCW"},"source":["## **Train Models**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cVcXMCIAKFXi"},"outputs":[],"source":["MODES_PARAMS = {0: (\"LSTM\", 0.0, False, 1.0, 1, 2.0, 5.0, 15),\n","                1: (\"LSTM_Dropout\", 0.35, False, 1.0, 7, 1.05, 5.0, 50),\n","                2: (\"GRU\", 0.0, True, 1.2, 1, 2.0, 2.0, 15),\n","                3: (\"GRU_Dropout\", 0.35, True, 1.2, 7, 1.08, 2.0, 50)}\n","\n","\n","def fit_model_for_mode(mode, file_name):\n","  (mode_name, p, use_gru, lr, decay_threshold, decay, max_norm, max_epochs) = MODES_PARAMS[mode]\n","\n","  if not os.path.exists(f\"{PATH}/Results/{mode_name}\"):\n","    os.makedirs(f\"{PATH}/Results/{mode_name}\")\n","  if not os.path.exists(f\"{PATH}/Models/{mode_name}\"):\n","      os.makedirs(f\"{PATH}/Models/{mode_name}\")\n","\n","  model = RNN(len(vocab_map), p=p, use_gru=use_gru).to(DEVICE)\n","  optimizer = SGD(model.parameters(), lr=lr)\n","  scheduler = lr_scheduler.LambdaLR(optimizer, lambda epoch: 1 if epoch < decay_threshold else decay**(-epoch+decay_threshold))\n","  return fit(model, train_set, test_set, optimizer, scheduler, max_norm, max_epochs=max_epochs, \n","             file_name=f\"{mode_name}/{file_name}\", save_model=True)\n","\n","\n","def evaluate_trained_model(mode, file_name):\n","  mode_name = MODES_PARAMS[mode][0]\n","  use_gru = mode > 1\n","  model = RNN(len(vocab_map), use_gru=use_gru).to(DEVICE)\n","  model.load_state_dict(torch.load(f\"{PATH}/Models/{mode_name}/{file_name}_best.pt\", map_location=DEVICE))\n","  train_loss = evaluation(model, train_set)\n","  valid_loss = evaluation(model, valid_set)\n","  test_loss = evaluation(model, test_set)\n","  print(f\"Model: {mode}, Train Perplexity: {np.exp(train_loss)}, Validation Perplexity: {np.exp(valid_loss)}, Test Perplexity: {np.exp(test_loss)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejZ98inHfLtq"},"outputs":[],"source":["# List of modes of models to train:\n","# 0: LSTM without dropout \n","# 1: LSTM with dropout \n","# 2: GRU without dropout \n","# 3: GRU with dropout \n","modes_to_run = [0, 1, 2, 3] \n","file_name = \"test_train\"\n","\n","for mode in modes_to_run:\n","  fit_model_for_mode(mode, file_name)\n","  evaluate_trained_model(mode, file_name)\n","\n"]},{"cell_type":"markdown","source":["## **Evaluations**\n","Get train and test evaluation for existing models"],"metadata":{"id":"QPGugS-S4aCk"}},{"cell_type":"code","source":["file_name = \"trained_model\"\n","for mode in MODES_PARAMS.keys():\n","  evaluate_trained_model(mode, file_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2mE-KtAR4l7V","executionInfo":{"status":"ok","timestamp":1651653144131,"user_tz":-180,"elapsed":1204717,"user":{"displayName":"Shani Jacobson","userId":"17202169386450009800"}},"outputId":"5840fe87-3f65-443a-e5a9-823b6d23b074"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"stream","name":"stdout","text":["Model: 0, Train Perplexity: 58.06171528545132, Validation Perplexity: 116.45095664102497, Test Perplexity: 112.0217984910468\n","Model: 1, Train Perplexity: 44.161830158557144, Validation Perplexity: 99.4926158715422, Test Perplexity: 94.49093709095733\n","Model: 2, Train Perplexity: 60.71044630672884, Validation Perplexity: 118.8719360926827, Test Perplexity: 114.92523756232262\n","Model: 3, Train Perplexity: 39.54581419538675, Validation Perplexity: 99.32161731855646, Test Perplexity: 95.04166000448642\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"EX2.ipynb","provenance":[{"file_id":"1VsbkH-ZNL-T216zsyApszLNjiPlvYfEt","timestamp":1651593308566},{"file_id":"14nq2E7sHsfuLCWcEc1U3kn9vYHdb3sHb","timestamp":1650455218099}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}