{"cells":[{"cell_type":"markdown","metadata":{"id":"AUsBR9Ez3iPG"},"source":["# **Lenet5 for Fashion-Mnist**"]},{"cell_type":"markdown","source":["## **Initialization**"],"metadata":{"id":"OZt2ZW6lqRL1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8K4jK-kxwlt5"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import datasets, transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18064,"status":"ok","timestamp":1647976561882,"user":{"displayName":"Shani Jacobson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17202169386450009800"},"user_tz":-120},"id":"GjR7EN3OcGZ2","outputId":"a70de3d8-bee9-45a0-ebec-6fcd9f5383cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","\n","PATH = \"/content/gdrive/MyDrive/ex1_313581803_314882861\"\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"PGMXQwzmiIzS"},"source":["### **Load Data**\n","Get mnist-fashion data from https://github.com/zalandoresearch/fashion-mnist."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KyFXJVuGbaY5"},"outputs":[],"source":["torch.manual_seed(42)\n","\n","total_train_set = datasets.FashionMNIST(\n","    root = f'{PATH}/Data/FashionMNIST',\n","    train = True,\n","    download = True,\n","    transform = transforms.Compose([\n","        transforms.ToTensor()                                 \n","    ])\n",")\n","test_set = datasets.FashionMNIST(\n","    root = f'{PATH}/Data/FashionMNIST',\n","    train = False,\n","    download = True,\n","    transform = transforms.Compose([\n","        transforms.ToTensor()                                 \n","    ])\n",")\n","\n","train_set , val_set = random_split(total_train_set, [50000, 10000])"]},{"cell_type":"markdown","metadata":{"id":"kt5VDFU5i4QP"},"source":["### **Network Architecture**\n","\n","Implement Lenet5 for Fashion-Mnist\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJ-Jjh_SnyiT"},"outputs":[],"source":["class FashionLeNet(nn.Module):\n","  def __init__(self, drop_p=0, bn_flag=False):\n","        super().__init__()\n","        self.batch_norm_layer = nn.BatchNorm2d(1) if bn_flag else None\n","        self.layer_1 = nn.Sequential(\n","            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.layer_2 = nn.Sequential(\n","            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.layer_3 = nn.Sequential(\n","            nn.Linear(in_features=16*5*5, out_features=120),\n","            nn.ReLU(),\n","            nn.Dropout(drop_p)\n","        )\n","        self.layer_4 = nn.Sequential(\n","            nn.Linear(in_features=120, out_features=84),\n","            nn.ReLU(),\n","            nn.Dropout(drop_p)\n","        )\n","        self.layer_5 = nn.Sequential(\n","            nn.Linear(in_features=84, out_features=10)\n","        )\n","        self.criterion = nn.CrossEntropyLoss(reduction='sum')\n","\n","  def forward(self, X):\n","        out = X if self.batch_norm_layer is None else self.batch_norm_layer(X)\n","        out = self.layer_1(out)\n","        out = self.layer_2(out)\n","        out = out.view(X.shape[0], -1)\n","        out = self.layer_3(out)\n","        out = self.layer_4(out)\n","        return self.layer_5(out)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"He2RC2SbbNQp"},"outputs":[],"source":["def fit(model, train_loader, val_loader, file_name, max_epochs=50, save_model=False):\n","    train_results = []\n","    val_results = []\n","    best_acc = 0\n","    model.train()\n","    for epoch in range(max_epochs):\n","        for _, (samples, labels) in enumerate(train_loader):\n","            # Forward pass\n","            pred = model(samples.to(DEVICE))\n","            batch_loss = model.criterion(pred, labels.to(DEVICE))\n","\n","            # Propagating the loss backward and optimizing the parameters\n","            optimizer.zero_grad()\n","            batch_loss.backward()\n","            optimizer.step()\n","\n","        train_results.append(evaluation(model, train_loader))\n","        val_results.append(evaluation(model, val_loader))\n","        if save_model and val_results[-1][1] > best_acc :\n","          torch.save(model.state_dict(),f\"{PATH}/Models/{file_name}.pt\")\n","          best_acc = val_results[-1][1]\n","        print(f\"epoch: {epoch}, train loss: {train_results[-1][0]}, validation loss: {val_results[-1][0]} \\\n","        train accuracy: {train_results[-1][1]}, validation accuracy: {val_results[-1][1]}\")\n","    generate_plots(train_results, val_results, file_name)\n","    return train_results[-1], val_results[-1]\n","\n","\n","def evaluation(model, data_loader):\n","    model.eval()\n","    corr = 0\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","      size = 0\n","      for _, (samples, labels) in enumerate(data_loader):\n","          size += len(labels)\n","          samples, labels = samples.to(DEVICE), labels.to(DEVICE)\n","          outputs = model(samples)  \n","          pred = torch.max(outputs.data, 1)[1]\n","          corr += (pred == labels).sum()\n","          total_loss += model.criterion(outputs, labels)\n","    return (float(total_loss) / size, float(corr) / size)\n","\n","\n","def generate_plots(train_results, val_results, file_name):\n","    if not os.path.exists(f\"{PATH}/Results\"):\n","      os.mkdir(f\"{PATH}/Results\")\n","    fig, axs = plt.subplots(1, 2, figsize=(50, 15))\n","    axs[0].set_xlabel(\"epoch\")\n","    axs[0].set_ylabel(\"loss\")\n","    axs[0].plot(np.array(train_results)[:, 0], label='training loss', linewidth=10.0)\n","    axs[0].plot(np.array(val_results)[:, 0], label='validation loss', linewidth=10.0)\n","    axs[0].set_title('Loss per epoch')\n","    axs[0].legend()\n","    axs[0].grid()\n","\n","    axs[1].set_xlabel(\"epoch\")\n","    axs[1].set_ylabel(\"accuracy\")\n","    axs[1].plot(np.array(train_results)[:, 1], label='training accuracy', linewidth=10.0)\n","    axs[1].plot(np.array(val_results)[:, 1], label='validation accuracy', linewidth=10.0)\n","    axs[1].set_title('Accuracy per epoch')\n","    axs[1].legend()\n","    axs[1].grid()\n","\n","    plt.savefig(f'{PATH}/Results/{file_name}.png', bbox_inches='tight')\n","    plt.close()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LINKb6vhlYlI"},"source":["## **Find Hyperparameters**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3NuW1HUk3emy"},"outputs":[],"source":["lr_list = np.arange(0.0005, 0.001, 0.00005)\n","batch_size = [128,256, 64] \n","results = {}\n","\n","MODE = \"Original\"\n","\n","if not os.path.exists(f\"{PATH}/Results/{MODE}\"):\n","    os.makedirs(f\"{PATH}/Results/{MODE}\")\n","if not os.path.exists(f\"{PATH}/Models/{MODE}\"):\n","    os.makedirs(f\"{PATH}/Models/{MODE}\")\n","\n","for bs in batch_size:\n","  train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n","  val_loader = DataLoader(val_set, batch_size=bs, shuffle=False)\n","  for lr in lr_list:\n","    model = FashionLeNet().to(DEVICE)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    print(f\"Start fit model with Batch Size: {bs}, learning_rate: {lr}\")\n","    results[(bs, lr)] = fit(model, train_loader, val_loader, f\"{MODE}/{bs}_{lr}\", save_model=False)\n","\n","np.save(f\"{PATH}\\res_dic\", results, allow_pickle=True)\n","   "]},{"cell_type":"markdown","metadata":{"id":"6HYfGUsR8n0R"},"source":["## **Train Models**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wv3QyV46MwP"},"outputs":[],"source":["LEARNING_RATE = 0.0007\n","BATCH_SIZE = 256\n","MODES = [\"Original\", \"Dropout\", \"L2\", \"BatchNormalization\"]\n","\n","\n","train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n","\n","def fit_model_for_mode(mode, model, optimizer):\n","  if not os.path.exists(f\"{PATH}/Results/{mode}\"):\n","    os.makedirs(f\"{PATH}/Results/{mode}\")\n","  if not os.path.exists(f\"{PATH}/Models/{mode}\"):\n","      os.makedirs(f\"{PATH}/Models/{mode}\")\n","  return fit(model, train_loader, val_loader, f\"{mode}/{BATCH_SIZE}_{LEARNING_RATE}\")\n","\n","def evaluate_trained_model(mode):\n","  if mode == 'BatchNormalization':\n","      model = FashionLeNet(bn_flag=True).to(DEVICE)\n","  else:\n","      model = FashionLeNet().to(DEVICE)\n","  model.load_state_dict(torch.load(f\"{PATH}/Models/{mode}/{BATCH_SIZE}_{LEARNING_RATE}.pt\"))\n","  _, train_accuracy = evaluation(model, train_loader)\n","  _, test_accuracy = evaluation(model, test_loader)\n","  print(f\"Model: {mode}, Train Accuracy: {train_accuracy}, Test Accuracy: {test_accuracy}\")\n"]},{"cell_type":"markdown","metadata":{"id":"5B636lqqLce2"},"source":["### **Original**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-bAb0dv4LmIe"},"outputs":[],"source":["mode = MODES[0]\n","\n","model = FashionLeNet().to(DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","results = fit_model_for_mode(mode, model, optimizer)\n","   "]},{"cell_type":"markdown","metadata":{"id":"m1x5ece2lzgA"},"source":["### **Dropout**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OG6EqBS_aJ-k"},"outputs":[],"source":["mode = MODES[1]\n","\n","model = FashionLeNet(drop_p=0.5).to(DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","results = fit_model_for_mode(mode, model, optimizer)   "]},{"cell_type":"markdown","metadata":{"id":"nM5TmBmNmBEL"},"source":["### **Weight Decay**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yajiZ48cmBEM"},"outputs":[],"source":["mode = MODES[2]\n","\n","model = FashionLeNet().to(DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n","results = fit_model_for_mode(mode, model, optimizer)"]},{"cell_type":"markdown","metadata":{"id":"9h0BS9ktmRnO"},"source":["### **Batch Normalization**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cuDUFdELmRnO"},"outputs":[],"source":["mode = MODES[3]\n","\n","model = FashionLeNet(bn_flag=True).to(DEVICE)\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","results = fit_model_for_mode(mode, model, optimizer)\n"]},{"cell_type":"markdown","source":["###**Evaluations**\n","Get train and test evaluation for existing models\n"],"metadata":{"id":"zWIZY-z6DKj3"}},{"cell_type":"code","source":["train_loader = DataLoader(total_train_set, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)\n","\n","for mode in MODES:\n","  evaluate_trained_model(mode)"],"metadata":{"id":"U_baLgwQDJRx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647977170052,"user_tz":-120,"elapsed":39003,"user":{"displayName":"Shani Jacobson","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17202169386450009800"}},"outputId":"ea92c062-512a-4023-db05-70eb63dfbad6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: Original, Train Accuracy: 0.9385166666666667, Test Accuracy: 0.9071\n","Model: Dropout, Train Accuracy: 0.9362666666666667, Test Accuracy: 0.9029\n","Model: L2, Train Accuracy: 0.9298833333333333, Test Accuracy: 0.9012\n","Model: BatchNormalization, Train Accuracy: 0.9318166666666666, Test Accuracy: 0.9013\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["9h0BS9ktmRnO","zWIZY-z6DKj3"],"name":"EX1.ipynb","provenance":[{"file_id":"1OOicE6TvabFmdK5LrsfSP1eYErkhD2b9","timestamp":1647973252534}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}